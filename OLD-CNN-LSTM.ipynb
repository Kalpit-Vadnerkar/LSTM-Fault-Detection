{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2f22e6-d253-4631-aed1-bd8f37f7e3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "\n",
    "main_directory = '/home/kvadner/Fault Detection/LSTM-Fault-Detection'\n",
    "data_folder = 'data'  # Adjust if your data folder has a different path\n",
    "model_name = 'CNN-LSTM-Huber-SGD-Batch200'\n",
    "model_folder = os.path.join(main_directory, 'model', model_name)\n",
    "\n",
    "# Set device for model training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39af84c1-5c26-4a03-ac96-c320aa3dee84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.label_fc = nn.Linear(num_classes, hidden_size)\n",
    "        self.output_fc = nn.Linear(hidden_size, num_features)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        label_out = self.label_fc(labels)\n",
    "        label_out = label_out.unsqueeze(1).repeat(1, lstm_out.size(1), 1)\n",
    "        combined = lstm_out + label_out\n",
    "        out = self.output_fc(combined)\n",
    "        return out\n",
    "\n",
    "# CNN-LSTM Model\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_size, num_layers):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.label_fc = nn.Linear(num_classes, hidden_size)\n",
    "        self.output_fc = nn.Linear(hidden_size, num_features)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        label_out = self.label_fc(labels)\n",
    "        label_out = label_out.unsqueeze(1).repeat(1, lstm_out.size(1), 1)\n",
    "        combined = lstm_out + label_out\n",
    "        out = self.output_fc(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7e348a-2dd2-45b9-bf79-df22383d98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, n_steps_in, n_steps_out):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "\n",
    "        self.n_steps_out = n_steps_out\n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_size=num_features, \n",
    "                                    hidden_size=hidden_size, \n",
    "                                    batch_first=True)\n",
    "        # Repeat vector\n",
    "        self.repeat = lambda x: x.repeat(1, n_steps_out, 1)\n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = nn.LSTM(input_size=hidden_size, \n",
    "                                    hidden_size=hidden_size, \n",
    "                                    batch_first=True, \n",
    "                                    return_sequences=True)\n",
    "        # TimeDistributed Dense\n",
    "        self.time_distributed = nn.Linear(hidden_size, num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        encoded, _ = self.encoder_lstm(x)\n",
    "        # Repeat vector\n",
    "        repeated = self.repeat(encoded[:, -1, :])\n",
    "        # Decoding\n",
    "        decoded, _ = self.decoder_lstm(repeated)\n",
    "        # TimeDistributed output\n",
    "        out = self.time_distributed(decoded)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e544b7-a97e-4689-b588-b63ad1f18add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, n_steps_in, n_steps_out):\n",
    "        super(SequentialLSTM, self).__init__()\n",
    "        \n",
    "        # First LSTM layer, returning sequences\n",
    "        self.lstm1 = nn.LSTM(input_size=num_features, hidden_size=hidden_size, \n",
    "                             batch_first=True, return_sequences=True)\n",
    "\n",
    "        # Second LSTM layer, not returning sequences\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                             batch_first=True)\n",
    "\n",
    "        # Dense layer\n",
    "        self.dense = nn.Linear(hidden_size, n_steps_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First LSTM\n",
    "        x, _ = self.lstm1(x)\n",
    "\n",
    "        # Second LSTM\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        # Only use the last output for the Dense layer\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Dense layer for output\n",
    "        out = self.dense(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a365102-cba0-4901-9124-d2d6f5abf0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_data = np.load(os.path.join(main_directory, data_folder, 'X_data.npy'))\n",
    "maneuver_labels = np.load(os.path.join(main_directory, data_folder, 'maneuver_labels.npy'))\n",
    "Y_data = np.load(os.path.join(main_directory, data_folder, 'Y_data.npy'))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "maneuver_labels_tensor = torch.tensor(maneuver_labels, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1ab075-f6e3-49db-8458-ea6ab82e5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, labels_train, labels_test, Y_train, Y_test = train_test_split(\n",
    "    X_tensor, maneuver_labels_tensor, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "train_dataset = TensorDataset(X_train, labels_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, labels_test, Y_test)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=120, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=120, shuffle=False)\n",
    "\n",
    "larger_batch_size = 200\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=larger_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=larger_batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1074d0-ec4c-4e81-b038-aa5050a34dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs, learning_rate):\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    #optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9)  # Using SGD with momentum\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    rmse_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for X_batch, labels_batch, Y_batch in train_loader:\n",
    "            X_batch = X_batch.permute(0, 2, 1)\n",
    "            Y_batch = Y_batch.permute(0, 2, 1)\n",
    "            \n",
    "            X_batch, labels_batch, Y_batch = X_batch.to(device), labels_batch.to(device), Y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch, labels_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, labels, Y in test_loader:\n",
    "                X = X.permute(0, 2, 1)\n",
    "                Y = Y.permute(0, 2, 1)\n",
    "                X, labels, Y = X.to(device), labels.to(device), Y.to(device)\n",
    "                test_outputs = model(X, labels)\n",
    "                test_loss = criterion(test_outputs, Y)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        # Calculating RMSE\n",
    "        rmse = np.sqrt(avg_test_loss)\n",
    "        rmse_values.append(rmse)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, RMSE: {rmse:.4f}')\n",
    "        scheduler.step(avg_test_loss)\n",
    "\n",
    "    return model, train_losses, test_losses, rmse_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d7cdc-18e3-441b-9f3c-52766b55a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Epoch 1/100, Train Loss: 0.2551, Test Loss: 0.1719, RMSE: 0.4146\n",
      "Epoch 2/100, Train Loss: 0.1343, Test Loss: 0.1041, RMSE: 0.3226\n",
      "Epoch 3/100, Train Loss: 0.0892, Test Loss: 0.0791, RMSE: 0.2813\n",
      "Epoch 4/100, Train Loss: 0.0738, Test Loss: 0.0699, RMSE: 0.2644\n",
      "Epoch 5/100, Train Loss: 0.0668, Test Loss: 0.0644, RMSE: 0.2538\n",
      "Epoch 6/100, Train Loss: 0.0623, Test Loss: 0.0607, RMSE: 0.2464\n",
      "Epoch 7/100, Train Loss: 0.0589, Test Loss: 0.0576, RMSE: 0.2400\n",
      "Epoch 8/100, Train Loss: 0.0558, Test Loss: 0.0543, RMSE: 0.2331\n",
      "Epoch 9/100, Train Loss: 0.0530, Test Loss: 0.0518, RMSE: 0.2276\n",
      "Epoch 10/100, Train Loss: 0.0509, Test Loss: 0.0500, RMSE: 0.2235\n",
      "Epoch 11/100, Train Loss: 0.0491, Test Loss: 0.0488, RMSE: 0.2208\n",
      "Epoch 12/100, Train Loss: 0.0476, Test Loss: 0.0471, RMSE: 0.2170\n",
      "Epoch 13/100, Train Loss: 0.0464, Test Loss: 0.0462, RMSE: 0.2149\n",
      "Epoch 14/100, Train Loss: 0.0453, Test Loss: 0.0449, RMSE: 0.2118\n",
      "Epoch 15/100, Train Loss: 0.0444, Test Loss: 0.0446, RMSE: 0.2113\n",
      "Epoch 16/100, Train Loss: 0.0436, Test Loss: 0.0434, RMSE: 0.2083\n",
      "Epoch 17/100, Train Loss: 0.0428, Test Loss: 0.0425, RMSE: 0.2062\n",
      "Epoch 18/100, Train Loss: 0.0422, Test Loss: 0.0418, RMSE: 0.2045\n",
      "Epoch 19/100, Train Loss: 0.0416, Test Loss: 0.0416, RMSE: 0.2039\n",
      "Epoch 20/100, Train Loss: 0.0410, Test Loss: 0.0410, RMSE: 0.2025\n",
      "Epoch 21/100, Train Loss: 0.0405, Test Loss: 0.0405, RMSE: 0.2012\n",
      "Epoch 22/100, Train Loss: 0.0400, Test Loss: 0.0402, RMSE: 0.2004\n",
      "Epoch 23/100, Train Loss: 0.0396, Test Loss: 0.0392, RMSE: 0.1981\n",
      "Epoch 24/100, Train Loss: 0.0391, Test Loss: 0.0393, RMSE: 0.1983\n",
      "Epoch 25/100, Train Loss: 0.0388, Test Loss: 0.0388, RMSE: 0.1969\n",
      "Epoch 26/100, Train Loss: 0.0384, Test Loss: 0.0386, RMSE: 0.1963\n",
      "Epoch 27/100, Train Loss: 0.0381, Test Loss: 0.0378, RMSE: 0.1945\n",
      "Epoch 28/100, Train Loss: 0.0378, Test Loss: 0.0378, RMSE: 0.1945\n",
      "Epoch 29/100, Train Loss: 0.0375, Test Loss: 0.0372, RMSE: 0.1929\n",
      "Epoch 30/100, Train Loss: 0.0372, Test Loss: 0.0373, RMSE: 0.1931\n",
      "Epoch 31/100, Train Loss: 0.0370, Test Loss: 0.0366, RMSE: 0.1913\n",
      "Epoch 32/100, Train Loss: 0.0367, Test Loss: 0.0367, RMSE: 0.1915\n",
      "Epoch 33/100, Train Loss: 0.0365, Test Loss: 0.0364, RMSE: 0.1908\n",
      "Epoch 34/100, Train Loss: 0.0363, Test Loss: 0.0365, RMSE: 0.1910\n",
      "Epoch 35/100, Train Loss: 0.0361, Test Loss: 0.0362, RMSE: 0.1904\n",
      "Epoch 36/100, Train Loss: 0.0359, Test Loss: 0.0362, RMSE: 0.1902\n",
      "Epoch 37/100, Train Loss: 0.0357, Test Loss: 0.0356, RMSE: 0.1888\n",
      "Epoch 38/100, Train Loss: 0.0355, Test Loss: 0.0353, RMSE: 0.1880\n",
      "Epoch 39/100, Train Loss: 0.0354, Test Loss: 0.0355, RMSE: 0.1883\n",
      "Epoch 40/100, Train Loss: 0.0352, Test Loss: 0.0351, RMSE: 0.1875\n",
      "Epoch 41/100, Train Loss: 0.0351, Test Loss: 0.0350, RMSE: 0.1871\n",
      "Epoch 42/100, Train Loss: 0.0349, Test Loss: 0.0348, RMSE: 0.1867\n",
      "Epoch 43/100, Train Loss: 0.0348, Test Loss: 0.0347, RMSE: 0.1864\n",
      "Epoch 44/100, Train Loss: 0.0347, Test Loss: 0.0346, RMSE: 0.1861\n",
      "Epoch 45/100, Train Loss: 0.0345, Test Loss: 0.0347, RMSE: 0.1863\n",
      "Epoch 46/100, Train Loss: 0.0344, Test Loss: 0.0348, RMSE: 0.1864\n",
      "Epoch 47/100, Train Loss: 0.0343, Test Loss: 0.0347, RMSE: 0.1863\n",
      "Epoch 48/100, Train Loss: 0.0342, Test Loss: 0.0341, RMSE: 0.1847\n",
      "Epoch 49/100, Train Loss: 0.0341, Test Loss: 0.0342, RMSE: 0.1850\n",
      "Epoch 50/100, Train Loss: 0.0340, Test Loss: 0.0338, RMSE: 0.1839\n",
      "Epoch 51/100, Train Loss: 0.0339, Test Loss: 0.0340, RMSE: 0.1844\n",
      "Epoch 52/100, Train Loss: 0.0338, Test Loss: 0.0340, RMSE: 0.1844\n",
      "Epoch 53/100, Train Loss: 0.0337, Test Loss: 0.0339, RMSE: 0.1840\n",
      "Epoch 54/100, Train Loss: 0.0336, Test Loss: 0.0337, RMSE: 0.1835\n",
      "Epoch 55/100, Train Loss: 0.0335, Test Loss: 0.0334, RMSE: 0.1828\n",
      "Epoch 56/100, Train Loss: 0.0334, Test Loss: 0.0333, RMSE: 0.1825\n",
      "Epoch 57/100, Train Loss: 0.0333, Test Loss: 0.0331, RMSE: 0.1821\n",
      "Epoch 58/100, Train Loss: 0.0332, Test Loss: 0.0336, RMSE: 0.1833\n",
      "Epoch 59/100, Train Loss: 0.0331, Test Loss: 0.0333, RMSE: 0.1825\n",
      "Epoch 60/100, Train Loss: 0.0330, Test Loss: 0.0330, RMSE: 0.1817\n",
      "Epoch 61/100, Train Loss: 0.0329, Test Loss: 0.0329, RMSE: 0.1814\n",
      "Epoch 62/100, Train Loss: 0.0329, Test Loss: 0.0330, RMSE: 0.1816\n",
      "Epoch 63/100, Train Loss: 0.0328, Test Loss: 0.0328, RMSE: 0.1811\n",
      "Epoch 64/100, Train Loss: 0.0327, Test Loss: 0.0327, RMSE: 0.1809\n",
      "Epoch 65/100, Train Loss: 0.0326, Test Loss: 0.0328, RMSE: 0.1811\n",
      "Epoch 66/100, Train Loss: 0.0326, Test Loss: 0.0326, RMSE: 0.1805\n",
      "Epoch 67/100, Train Loss: 0.0325, Test Loss: 0.0324, RMSE: 0.1800\n",
      "Epoch 68/100, Train Loss: 0.0324, Test Loss: 0.0328, RMSE: 0.1812\n",
      "Epoch 69/100, Train Loss: 0.0323, Test Loss: 0.0323, RMSE: 0.1798\n",
      "Epoch 70/100, Train Loss: 0.0323, Test Loss: 0.0323, RMSE: 0.1797\n",
      "Epoch 71/100, Train Loss: 0.0322, Test Loss: 0.0323, RMSE: 0.1796\n",
      "Epoch 72/100, Train Loss: 0.0321, Test Loss: 0.0321, RMSE: 0.1793\n",
      "Epoch 73/100, Train Loss: 0.0321, Test Loss: 0.0320, RMSE: 0.1789\n",
      "Epoch 74/100, Train Loss: 0.0320, Test Loss: 0.0320, RMSE: 0.1790\n",
      "Epoch 75/100, Train Loss: 0.0319, Test Loss: 0.0319, RMSE: 0.1785\n",
      "Epoch 76/100, Train Loss: 0.0319, Test Loss: 0.0321, RMSE: 0.1791\n",
      "Epoch 77/100, Train Loss: 0.0318, Test Loss: 0.0317, RMSE: 0.1781\n",
      "Epoch 78/100, Train Loss: 0.0317, Test Loss: 0.0318, RMSE: 0.1784\n",
      "Epoch 79/100, Train Loss: 0.0317, Test Loss: 0.0317, RMSE: 0.1779\n",
      "Epoch 80/100, Train Loss: 0.0316, Test Loss: 0.0316, RMSE: 0.1779\n",
      "Epoch 81/100, Train Loss: 0.0316, Test Loss: 0.0316, RMSE: 0.1778\n",
      "Epoch 82/100, Train Loss: 0.0315, Test Loss: 0.0317, RMSE: 0.1780\n",
      "Epoch 83/100, Train Loss: 0.0314, Test Loss: 0.0316, RMSE: 0.1777\n"
     ]
    }
   ],
   "source": [
    "# Training the CNN-LSTM model and obtaining metrics\n",
    "cnn_lstm_model = CNNLSTMModel(num_features=8, num_classes=7, hidden_size=50, num_layers=2)\n",
    "\n",
    "# Check for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    cnn_lstm_model = nn.DataParallel(cnn_lstm_model)\n",
    "\n",
    "cnn_lstm_model.to(device)\n",
    "\n",
    "# Training the model (as before)\n",
    "trained_cnn_lstm_model, cnn_lstm_train_losses, cnn_lstm_test_losses, cnn_lstm_rmse = train_model(\n",
    "    cnn_lstm_model, train_loader, test_loader, epochs=100, learning_rate=0.001)\n",
    "\n",
    "#trained_cnn_lstm_model, cnn_lstm_train_losses, cnn_lstm_test_losses, cnn_lstm_rmse = train_model(\n",
    "#    cnn_lstm_model, train_loader, test_loader, epochs=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa62ac-ebf3-4172-9efd-bff8253d4435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(trained_cnn_lstm_model.state_dict(), os.path.join(main_directory, model_folder, 'cnn_lstm_model_100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4426705-1786-4045-9254-963d09723a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting and saving Loss graph\n",
    "plt.figure()\n",
    "plt.plot(cnn_lstm_train_losses, label='Train Loss')\n",
    "plt.plot(cnn_lstm_test_losses, label='Test Loss')\n",
    "plt.title('CNN-LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(main_directory, model_folder, 'cnn_lstm_model_MSE.png'))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cnn_lstm_rmse, label='Test RMSE')\n",
    "plt.title('CNN-LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(main_directory, model_folder, 'cnn_lstm_model_RMSE.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05041a7-6b0b-4797-8dd3-e17524de42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the CSV file\n",
    "csv_file_path = os.path.join(main_directory, model_folder, 'cnn_lstm_model_results.csv')\n",
    "\n",
    "# Writing metrics to CSV\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Train Loss', 'Test Loss', 'RMSE'])\n",
    "    for epoch in range(len(cnn_lstm_train_losses)):\n",
    "        writer.writerow([epoch + 1, cnn_lstm_train_losses[epoch], cnn_lstm_test_losses[epoch], cnn_lstm_rmse[epoch]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d58564-522f-42e9-9601-8dfdfdc81d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def loadScaler(variable):\n",
    "    filename = 'scaler_' + str(variable) + '.pkl'\n",
    "    with open(os.path.join(main_directory, scaler_folder, filename), 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "scaler_folder = 'Scalers'\n",
    "features = ['theta', 'v', 'r', 'ay', 'force', 'u','xu', 'xy']\n",
    "# load scalers \n",
    "scalers = {variable: loadScaler(variable) for variable in features}\n",
    "# load model\n",
    "#model = LSTMModel(num_features=8, num_classes=7, hidden_size=100, num_layers=2).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(main_directory, model_folder, 'stacked_lstm_model_200.pth')))\n",
    "\n",
    "#model = LSTMModel(num_features=8, num_classes=7, hidden_size=100, num_layers=2).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(main_directory, model_folder, 'lstm_model.pth')))\n",
    "\n",
    "#model = CNNLSTMModel(num_features=8, num_classes=7, hidden_size=100, num_layers=2).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(main_directory, model_folder, 'cnn_lstm_model_200.pth')))\n",
    "\n",
    "#model = CNNLSTMModel(num_features=8, num_classes=7, hidden_size=50, num_layers=2).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(main_directory, model_folder, 'cnn_lstm_model_100_Epochs.pth')))\n",
    "\n",
    "model = CNNLSTMModel(num_features=8, num_classes=7, hidden_size=50, num_layers=2).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "#print(os.path.join(main_directory, model_folder, 'cnn_lstm_model_100.pth'))\n",
    "model.load_state_dict(torch.load(os.path.join(main_directory, model_folder, 'cnn_lstm_model_100.pth')))\n",
    "\n",
    "#print(X_data[0])\n",
    "#print(Y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38829ec-a2bc-4007-9aa3-cf929d0fc626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select examples based on given indices\n",
    "indices = [i for i in range(400, 800, 25)]\n",
    "selected_X = X_tensor[indices]\n",
    "selected_Y = Y_tensor[indices]\n",
    "selected_labels = maneuver_labels_tensor[indices]\n",
    "\n",
    "selected_X = selected_X.permute(0, 2, 1)\n",
    "selected_Y = selected_Y.permute(0, 2, 1)\n",
    "\n",
    "#model = model.cpu()\n",
    "#selected_X = selected_X.cpu()\n",
    "#selected_labels = selected_labels.cpu()\n",
    "#selected_Y = selected_Y.cpu()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Move the model to the specified device\n",
    "\n",
    "# Also move the input data to the same device\n",
    "selected_X = selected_X.to(device)\n",
    "selected_labels = selected_labels.to(device)\n",
    "selected_Y = selected_Y.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_Y = model(selected_X, selected_labels)\n",
    "    \n",
    "# Move the predictions to CPU for processing with NumPy\n",
    "predicted_Y = predicted_Y.cpu()\n",
    "selected_Y = selected_Y.cpu()\n",
    "\n",
    "# Inverse scaling\n",
    "inverse_scaled_predicted = []\n",
    "inverse_scaled_true = []\n",
    "for i, example in enumerate(predicted_Y):\n",
    "    inverse_scaled_example = []\n",
    "    for j, feature in enumerate(features):\n",
    "        inverse_scaled_example.append(scalers[feature].inverse_transform(example[:, j].numpy().reshape(-1, 1)))\n",
    "    inverse_scaled_predicted.append(np.hstack(inverse_scaled_example))\n",
    "    inverse_scaled_true.append(np.hstack([scalers[feature].inverse_transform(selected_Y[i][:, j].numpy().reshape(-1, 1)) for j, feature in enumerate(features)]))\n",
    "\n",
    "# Plotting\n",
    "# Concatenate all sequences for each feature\n",
    "concatenated_true = np.concatenate([inverse_scaled_true[i] for i in range(len(inverse_scaled_true))], axis=0)\n",
    "concatenated_pred = np.concatenate([inverse_scaled_predicted[i] for i in range(len(inverse_scaled_predicted))], axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 15))\n",
    "for j in range(len(features)):\n",
    "    plt.subplot(len(features), 1, j + 1)\n",
    "    plt.plot(concatenated_true[:, j], label=f\"True {features[j]}\")\n",
    "    plt.plot(concatenated_pred[:, j], label=f\"Predicted {features[j]}\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Feature: {features[j]} - True vs Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf192b5-0a34-4c09-8836-ab3215550ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
